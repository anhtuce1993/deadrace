
################################################################################

                               QCD SPEED TESTS

################################################################################


PROGRAMS

The benchmark programs are provided in source form and must be
compiled by the user on the machine that is to be tested. The
subdirectories in this directory are

devel              Check programs that allow the correct functioning of the
                   programs in the modules directory to be verified

doc                Documentation files for lattice QCD experts

include            Header files

main               Benchmark programs

modules            Programs called by the benchmark programs

Normally it will not be necessary to execute the check programs in the
devel directory, but they are included in case there are any doubts
that the programs were correctly compiled.


STANDARDS COMPLIANCE
 
All programs parallelize in 0,1,2,3 or 4 dimensions, depending on what
is specified at compilation time. They are highly optimized for
machines with current Intel or AMD processors, but will run correctly
on any system that complies with the ISO C89 (formerly ANSI C) and the
MPI 1.2 standards.

For the purpose of testing, the programs can also be run on a desktop
or laptop computer. All what is needed is a C compiler and a local MPI
installation such as MPICH.


PROGRAM PARAMETERS

The only adjustable parameters are the sizes L0,L1,L2,L3 of the local
lattice and the numbers NPROC0,NPROC1,NPROC2,NPROC3 of MPI processes
in the four space-time directions. The values of these parameters are
defined at the top of the file include/global.h.

It is possible to set the process number NPROCx in direction x to 1 in
which case the program will not parallelize in that direction. NPROCx
must otherwise be even. The sizes Lx must also be even and not smaller
than 4.

The total lattice is then of size

  (NPROC0*L0)x(NPROC1*L1)x(NPROC2*L2)x(NPROC3*L3)

and the total number of MPI processes is 

  NPROC=NPROC0*NPROC1*NPROC2*NPROC3

Full details on the lattice geometry and the layout of the basic
fields are given in the file README.global.


COMPILATION

The compilation of the programs requires an ISO C89 compliant compiler
and a compatible MPI installation that complies with the MPI standard
1.2 (or later). An MPI implementation is needed even if the programs
are to run on a single processing unit.

In the main and devel directories, a GNU-style Makefile is included
which compiles and links the programs using a specified compiler and
MPI installation (just type "make" to compile everything; "make clean"
removes the files generated by "make"). The compiler options can be
set by editing the CFLAGS line in the Makefiles.

The Makefiles assume that the environment variables

  GCC             GNU C compiler command [Example: /usr/bin/gcc]

  MPIR_HOME       MPI home directory [Example: /usr/local/share/mpich]

are defined. All programs are compiled using the $MPIR_HOME/bin/mpicc
command. The compiler options that can be set in the CFLAGS line
depend on which C compiler is invoked by the mpicc command.


SSE ACCELERATION

Current Intel and AMD processors are able to perform arithmetic
operations on vectors of 4 single-precision or 2 double-precision
floating-point numbers in just one or two machine cycles, using SSE
(Streaming SIMD Extension) arithmetic units. The arithmetic performed
in the SSE registers fully complies with the IEEE 754 standard, i.e.
no compromises are made at that level.

The associated vector instruction set was developed in steps, denoted
SSE, SSE2, SSE3, etc., where each set added some functionalities to
the previous one. QCD programs can profit from these extended
capabilities through the use of an optimizing compiler or, more
effectively, by including SSE inline-assembly code or so-called
intrinsics.

Some programs in the module directories include SSE inline-assembly
code which can be activated at compilation time by defining one of the
macros SSE, SSE2 or SSE3 (as appropriate). Otherwise (that is, if none
of these macros is defined) the programs are ISO C89 compliant and will
compile and execute correctly on practically any computer.

If the SSE inline-assembly code is to be used, the programs must be
compiled with the GNU C compiler version 3.0 or later. On systems
where the recommended mpicc command invokes another compiler, the GNU
C can still be used to compile each source file separately, taking
care that the proper mpi.h file is included. A system-compliant
executable is then obtained by linking the object files using the
recommended mpicc command.

In the compile step, the usual GNU C compiler options may be switched
on plus one of the following:

  -DSSE    Enables the use of SSE instructions
 
  -DSSE2   Enables the use of SSE2 instructions (implies -DSSE)

  -DSSE3   Enables the use of SSE3 instructions (implies -DSSE2) 

In addition, SSE memory prefetch instructions will be used if one of
the following options is specified:

  -DP4     Assume that prefetch instructions fetch 128 bytes at a time
           (Pentium 4 and related Xeons) 

  -DPM     Assume that prefetch instructions fetch 64 bytes at a time
           (Athlon, Opteron, Pentium M, Core, Core 2 and related Xeons) 

  -DP3     Assume that prefetch instructions fetch 32 bytes at a time
           (Pentium III) 

These options have an effect only if the use of SSE instructions is
enabled. 

If the specified instruction set is not supported by the processors,
the programs will terminate abnormally (after a few minutes at most)
with an error message saying that an unknown instruction was
encountered. Otherwise they always execute correctly, independently of
whether the assumed and the actual number of bytes prefetched by the
processors match. However, the programs will normally be faster if the
correct prefetch option is chosen.

Note that the use of SSE instructions may have to be enabled by adding
-msse, -msse2 or -msse3 to the compiler flags, respectively, if 32 bit
code is generated (these options are switched on by default if 64 bit
code is generated).

On recent x86 machines with AMD Opteron or Intel Xeon processors, for 
example, the recommended compiler flags are

  for 64 bit code generation: 

    -std=c89 -O -DSSE3 -DPM

  for 32 bit code generation: 

    -std=c89 -O -m32 -malign-double -msse3 -DSSE3 -DPM

More aggressive optimization levels such as -O2 and -O3 tend to have
little effect on the execution speed of the programs, but the risk of
generating wrong code is higher and it is therefore recommended to use
only the first-level optimization.


RUNNING THE BENCHMARK PROGRAMS

The QCD speed tests thus require the following steps to be taken:

1. Edit the file include/global.h and set the lattice sizes and
   process numbers to the desired values.

2. Set the environment variables MPIR_HOME and GCC to the proper
   paths. Change to the directory "main". Set the desired compiler
   options by editing the CFLAGS line in the Makefile.

3. Type ./make at the command prompt. If all goes well this should
   produce the executables "time1" and "time2".

4. Edit the MPI machine file and insert the list of nodes that are
   to be used in the tests.

5. Run the executables time1 and time2 using the appropriate mpirun
   command. The number of MPI processes specified on the command line
   must match the total number NPROC of processes set in the global.h
   file.

The execution time required for the tests should normally be a few
minutes only.


TEST RESULTS

The benchmark programs write the results of the speed tests to the
files time1.log and time2.log in the main directory. 

The programs time1 and time2 perform the same speed tests using
single-precision and double-precision arithmetic respectively. More
precisely, timings are taken for the following three basic operations:

 norm_square         Computes the square of the (global) norm of a 
                     given quark field 

 mulc_spinor_add     Multiplies a given quark field by a complex 
                     number and adds the field to another field

 Qhat                Application of the even-odd preconditioned 
                     Wilson-Dirac operator to a given quark field

The average execution times and corresponding Mflop rates per
MPI process are printed to the output files.

The execution time required per iteration of the conjugate-gradient
solver of the lattice Dirac equation is then estimated by adding the
execution times for 2 applications of norm_square, 3 applications of
mulc_spinor_add and 2 applications of Qhat. This is considered to be a
representative performance figure for lattice QCD computations and is
referred to as a "Synthetic QCD Speed Test" in the log files. The
total throughput quoted there is equal to the average speed per MPI
process times the number of processes.


LICENCE

The programs may be used under the terms of the GNU General Public
Licence (GPL).


BUG REPORTS

Please send a report to <luscher@mail.cern.ch> if a bug is detected.
